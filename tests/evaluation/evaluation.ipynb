{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fdd321a",
   "metadata": {},
   "source": [
    "# Retrieval Evaluation (Evidence-Based)\n",
    "\n",
    "This notebook evaluates the retrieval component of our RAG system using\n",
    "**evidence satisfaction** rather than flat relevance.\n",
    "\n",
    "The same evaluation logic is reused across different categories of questions:\n",
    "- Conceptual / Declarative\n",
    "- Structural / Symbolic\n",
    "- Compound / Multi-evidence\n",
    "\n",
    "Each category is evaluated independently by passing a different gold file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adb530b",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "We evaluate retrieval performance using **Precision@K** and **Recall@K**, defined under an **evidence-based relevance** setting rather than flat document matching.\n",
    "\n",
    "In this context, a question is considered *successfully answered* if the retrieved chunks satisfy the logical evidence required to answer the question.\n",
    "\n",
    "---\n",
    "\n",
    "### Precision@K\n",
    "\n",
    "**Plain-English definition:**\n",
    "\n",
    "> Precision measures **how many of the retrieved documents are actually useful** for answering the question.\n",
    "\n",
    "In other words:\n",
    "\n",
    "> *Of what the retriever returned, how much contributed toward answering the question?*\n",
    "\n",
    "**Formal definition:**\n",
    "\n",
    "Let:\n",
    "\n",
    "* **R** be the set of documents that are relevant for answering the question\n",
    "* **K** be the set of top-K retrieved documents\n",
    "\n",
    "```\n",
    "Precision@K = |R ∩ K| / |K|\n",
    "```\n",
    "\n",
    "Because most questions require only a small amount of evidence (often one sufficient chunk or set of chunks), precision values are naturally lower for larger values of K.\n",
    "\n",
    "---\n",
    "\n",
    "### Recall@K\n",
    "\n",
    "**Plain-English definition:**\n",
    "\n",
    "> Recall measures **how many of the documents that are needed to answer the question were successfully retrieved**.\n",
    "\n",
    "In other words:\n",
    "\n",
    "> *Of what was needed to answer the question, how much did the retriever manage to find?*\n",
    "\n",
    "**Formal definition:**\n",
    "\n",
    "Let:\n",
    "\n",
    "* **R** be the set of all documents required to answer the question\n",
    "* **K** be the set of top-K retrieved documents\n",
    "\n",
    "```\n",
    "Recall@K = |R ∩ K| / |R|\n",
    "```\n",
    "\n",
    "In this evaluation, recall is treated as **binary per question**: a question has recall 1 if the retrieved documents satisfy the evidence requirements, and 0 otherwise. This reflects whether the retriever succeeded in providing *sufficient evidence* to answer the question at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1fb71049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to PYTHONPATH: D:\\GitHub Doc Bot (Capstone Project 1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Move up until we find the project root (the folder containing 'backend')\n",
    "current = Path.cwd().resolve()\n",
    "\n",
    "for parent in [current] + list(current.parents):\n",
    "    if (parent / \"backend\").exists():\n",
    "        PROJECT_ROOT = parent\n",
    "        break\n",
    "else:\n",
    "    raise RuntimeError(\"Could not find project root containing 'backend'\")\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"Project root added to PYTHONPATH:\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "842ce4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "from backend.tasks.query.rag.embed_query import embed_query\n",
    "from backend.tasks.query.rag.retrieve import retrieve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc73f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_chunk(chunk: Dict) -> Tuple[str, int]:\n",
    "    return chunk[\"file_path\"], chunk[\"chunk_index\"]\n",
    "\n",
    "\n",
    "def evaluate_evidence(node: Dict, retrieved_set: set) -> bool:\n",
    "    if \"chunk\" in node:\n",
    "        return normalize_chunk(node[\"chunk\"]) in retrieved_set\n",
    "\n",
    "    op = node[\"op\"]\n",
    "    children = node[\"children\"]\n",
    "\n",
    "    if op == \"AND\":\n",
    "        return all(evaluate_evidence(c, retrieved_set) for c in children)\n",
    "\n",
    "    if op == \"OR\":\n",
    "        return any(evaluate_evidence(c, retrieved_set) for c in children)\n",
    "\n",
    "    if op == \"K_OF_N\":\n",
    "        k = node[\"k\"]\n",
    "        return sum(evaluate_evidence(c, retrieved_set) for c in children) >= k\n",
    "\n",
    "    raise ValueError(f\"Unknown operator: {op}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4e9366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\".\").resolve()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "\n",
    "\n",
    "def evaluate_file(\n",
    "    filename: str,\n",
    "    top_k: int = 5,\n",
    "    verbose: bool = True\n",
    "):\n",
    "    gold_file = DATA_DIR / filename\n",
    "\n",
    "    with open(gold_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        gold = json.load(f)\n",
    "\n",
    "    satisfied_count = 0\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for item in gold:\n",
    "        question = item[\"question\"]\n",
    "        evidence = item[\"evidence\"]\n",
    "\n",
    "        query_embedding = embed_query(question)\n",
    "        retrieved = retrieve(query_embedding, top_k=top_k)\n",
    "\n",
    "        retrieved_set = {\n",
    "            normalize_chunk(c) for c in retrieved\n",
    "        }\n",
    "\n",
    "        satisfied = evaluate_evidence(evidence, retrieved_set)\n",
    "\n",
    "        precision = (1 / top_k) if satisfied else 0.0\n",
    "        recall = 1.0 if satisfied else 0.0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Q: {question}\")\n",
    "            print(f\"  Evidence satisfied: {satisfied}\")\n",
    "            print(f\"  Precision@{top_k}: {precision:.3f}\")\n",
    "            print(f\"  Recall@{top_k}:    {recall:.3f}\")\n",
    "            print('-' * 60)\n",
    "\n",
    "    avg_precision = sum(precisions) / len(precisions) if precisions else 0.0\n",
    "    avg_recall = sum(recalls) / len(recalls) if recalls else 0.0\n",
    "\n",
    "    print(\"==== Summary ====\")\n",
    "    print(f\"Questions evaluated: {len(gold)}\")\n",
    "    print(f\"Average Precision@{top_k}: {avg_precision:.3f}\")\n",
    "    print(f\"Average Recall@{top_k}:    {avg_recall:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"questions\": len(gold),\n",
    "        \"avg_precision\": avg_precision,\n",
    "        \"avg_recall\": avg_recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0cbf75",
   "metadata": {},
   "source": [
    "## Conceptual / Declarative Questions\n",
    "\n",
    "These questions focus on descriptive, natural-language facts such as\n",
    "frameworks, databases, and configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1aade48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Which web framework is used to build the API?\n",
      "  Evidence satisfied: True\n",
      "  Precision@5: 0.200\n",
      "  Recall@5:    1.000\n",
      "------------------------------------------------------------\n",
      "Q: Where is the FastAPI application instance created?\n",
      "  Evidence satisfied: True\n",
      "  Precision@5: 0.200\n",
      "  Recall@5:    1.000\n",
      "------------------------------------------------------------\n",
      "Q: Which database is used for persistent storage?\n",
      "  Evidence satisfied: True\n",
      "  Precision@5: 0.200\n",
      "  Recall@5:    1.000\n",
      "------------------------------------------------------------\n",
      "Q: Which environment variable stores the MongoDB connection URI?\n",
      "  Evidence satisfied: True\n",
      "  Precision@5: 0.200\n",
      "  Recall@5:    1.000\n",
      "------------------------------------------------------------\n",
      "Q: What programming language is used to implement the backend of this project?\n",
      "  Evidence satisfied: True\n",
      "  Precision@5: 0.200\n",
      "  Recall@5:    1.000\n",
      "------------------------------------------------------------\n",
      "Q: What database technology does the application depend on?\n",
      "  Evidence satisfied: True\n",
      "  Precision@5: 0.200\n",
      "  Recall@5:    1.000\n",
      "------------------------------------------------------------\n",
      "Q: How is the application started during development?\n",
      "  Evidence satisfied: True\n",
      "  Precision@5: 0.200\n",
      "  Recall@5:    1.000\n",
      "------------------------------------------------------------\n",
      "Q: What type of application does this project implement (e.g., notes app, CRUD service)?\n",
      "  Evidence satisfied: True\n",
      "  Precision@5: 0.200\n",
      "  Recall@5:    1.000\n",
      "------------------------------------------------------------\n",
      "Q: What external service must be running for the application to function correctly?\n",
      "  Evidence satisfied: False\n",
      "  Precision@5: 0.000\n",
      "  Recall@5:    0.000\n",
      "------------------------------------------------------------\n",
      "Q: Which framework is responsible for handling HTTP requests and responses?\n",
      "  Evidence satisfied: False\n",
      "  Precision@5: 0.000\n",
      "  Recall@5:    0.000\n",
      "------------------------------------------------------------\n",
      "==== Summary ====\n",
      "Questions evaluated: 10\n",
      "Average Precision@5: 0.160\n",
      "Average Recall@5:    0.800\n"
     ]
    }
   ],
   "source": [
    "conceptual_results = evaluate_file(\n",
    "    filename=\"conceptual_gold.json\",\n",
    "    top_k=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7e940a",
   "metadata": {},
   "source": [
    "## Structural / Symbolic Questions\n",
    "\n",
    "These questions depend on code structure rather than descriptive text,\n",
    "including route decorators, schemas, and endpoint definitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd4ff349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Which file contains the main API route definitions?\n",
      "  Evidence satisfied: False\n",
      "  Precision@5: 0.000\n",
      "  Recall@5:    0.000\n",
      "------------------------------------------------------------\n",
      "Q: Where is the database client initialized?\n",
      "  Evidence satisfied: True\n",
      "  Precision@5: 0.200\n",
      "  Recall@5:    1.000\n",
      "------------------------------------------------------------\n",
      "Q: Which library is used to define request and response schemas?\n",
      "  Evidence satisfied: False\n",
      "  Precision@5: 0.000\n",
      "  Recall@5:    0.000\n",
      "------------------------------------------------------------\n",
      "Q: Where is the data model for notes defined?\n",
      "  Evidence satisfied: False\n",
      "  Precision@5: 0.000\n",
      "  Recall@5:    0.000\n",
      "------------------------------------------------------------\n",
      "Q: Which endpoint is responsible for creating a new note?\n",
      "  Evidence satisfied: True\n",
      "  Precision@5: 0.200\n",
      "  Recall@5:    1.000\n",
      "------------------------------------------------------------\n",
      "Q: Which endpoint deletes an existing note?\n",
      "  Evidence satisfied: True\n",
      "  Precision@5: 0.200\n",
      "  Recall@5:    1.000\n",
      "------------------------------------------------------------\n",
      "Q: Which HTTP method is used to retrieve all notes?\n",
      "  Evidence satisfied: False\n",
      "  Precision@5: 0.000\n",
      "  Recall@5:    0.000\n",
      "------------------------------------------------------------\n",
      "Q: Where is the endpoint for updating an existing note implemented?\n",
      "  Evidence satisfied: True\n",
      "  Precision@5: 0.200\n",
      "  Recall@5:    1.000\n",
      "------------------------------------------------------------\n",
      "Q: Which file defines the schema used to validate note data?\n",
      "  Evidence satisfied: False\n",
      "  Precision@5: 0.000\n",
      "  Recall@5:    0.000\n",
      "------------------------------------------------------------\n",
      "Q: Which part of the codebase handles interactions with the MongoDB collection?\n",
      "  Evidence satisfied: False\n",
      "  Precision@5: 0.000\n",
      "  Recall@5:    0.000\n",
      "------------------------------------------------------------\n",
      "==== Summary ====\n",
      "Questions evaluated: 10\n",
      "Average Precision@5: 0.080\n",
      "Average Recall@5:    0.400\n"
     ]
    }
   ],
   "source": [
    "structural_results = evaluate_file(\n",
    "    filename=\"structural_gold.json\",\n",
    "    top_k=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32d852a",
   "metadata": {},
   "source": [
    "## Compound / Multi-evidence Questions\n",
    "\n",
    "These questions require combining information across multiple chunks,\n",
    "often with logical relationships (AND / OR / K-of-N).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2cb39c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Which file defines the FastAPI app and also registers API routes?\n",
      "  Evidence satisfied: False\n",
      "  Precision@5: 0.000\n",
      "  Recall@5:    0.000\n",
      "------------------------------------------------------------\n",
      "Q: Which file or documentation source specifies the database used by the application?\n",
      "  Evidence satisfied: True\n",
      "  Precision@5: 0.200\n",
      "  Recall@5:    1.000\n",
      "------------------------------------------------------------\n",
      "Q: Where is the MongoDB connection configured and where is it consumed?\n",
      "  Evidence satisfied: False\n",
      "  Precision@5: 0.000\n",
      "  Recall@5:    0.000\n",
      "------------------------------------------------------------\n",
      "Q: Which files together define the data schema and validation logic for notes?\n",
      "  Evidence satisfied: False\n",
      "  Precision@5: 0.000\n",
      "  Recall@5:    0.000\n",
      "------------------------------------------------------------\n",
      "Q: Which endpoint definitions expose CRUD functionality for notes?\n",
      "  Evidence satisfied: False\n",
      "  Precision@5: 0.000\n",
      "  Recall@5:    0.000\n",
      "------------------------------------------------------------\n",
      "Q: Which source specifies how to start the application and which framework it runs on?\n",
      "  Evidence satisfied: False\n",
      "  Precision@5: 0.000\n",
      "  Recall@5:    0.000\n",
      "------------------------------------------------------------\n",
      "Q: Which code locations demonstrate that MongoDB is required for the application to function?\n",
      "  Evidence satisfied: False\n",
      "  Precision@5: 0.000\n",
      "  Recall@5:    0.000\n",
      "------------------------------------------------------------\n",
      "Q: Where are HTTP methods for retrieving and updating notes defined?\n",
      "  Evidence satisfied: False\n",
      "  Precision@5: 0.000\n",
      "  Recall@5:    0.000\n",
      "------------------------------------------------------------\n",
      "Q: Which parts of the project define API behavior versus database behavior?\n",
      "  Evidence satisfied: False\n",
      "  Precision@5: 0.000\n",
      "  Recall@5:    0.000\n",
      "------------------------------------------------------------\n",
      "Q: Which files provide sufficient information to understand the overall project structure?\n",
      "  Evidence satisfied: False\n",
      "  Precision@5: 0.000\n",
      "  Recall@5:    0.000\n",
      "------------------------------------------------------------\n",
      "==== Summary ====\n",
      "Questions evaluated: 10\n",
      "Average Precision@5: 0.020\n",
      "Average Recall@5:    0.100\n"
     ]
    }
   ],
   "source": [
    "compound_results = evaluate_file(\n",
    "    filename=\"compound_gold.json\",\n",
    "    top_k=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c2df3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>avg_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Conceptual</td>\n",
       "      <td>10</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Structural</td>\n",
       "      <td>10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Compound</td>\n",
       "      <td>10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category  questions  avg_precision  avg_recall\n",
       "0  Conceptual         10           0.16         0.8\n",
       "1  Structural         10           0.08         0.4\n",
       "2    Compound         10           0.02         0.1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary = pd.DataFrame([\n",
    "    {\"Category\": \"Conceptual\", **conceptual_results},\n",
    "    {\"Category\": \"Structural\", **structural_results},\n",
    "    {\"Category\": \"Compound\", **compound_results},\n",
    "])\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85d59c6",
   "metadata": {},
   "source": [
    "## Key Observations\n",
    "\n",
    "- Embedding-based retrieval performs well for conceptual questions with\n",
    "  strong natural-language signal.\n",
    "- Performance degrades for structural questions due to the lack of\n",
    "  code-aware or syntax-aware retrieval.\n",
    "- Compound questions further expose the absence of multi-hop or iterative\n",
    "  retrieval mechanisms.\n",
    "\n",
    "These results reflect inherent limitations of embedding-only retrieval,\n",
    "not implementation errors.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
